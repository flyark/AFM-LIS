{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Interaction Score calculation\n",
    "# It's designed for ColabFold-derived outputs (json and pdb files)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import statistics\n",
    "import numpy as np\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "\n",
    "def calculate_pae(pdb_file_path: str, print_results: bool = True, pae_cutoff: float = 12.0):\n",
    "    parser = PDB.PDBParser()\n",
    "\n",
    "    # Extracting information from the pdb_file_path\n",
    "    file_name = pdb_file_path.split(\"/\")[-1]\n",
    "    data_folder = pdb_file_path.split(\"/\")[-2]\n",
    "    \n",
    "    # Check if the file name contains 'rank' \n",
    "    if 'rank' not in file_name:\n",
    "        print(f\"Skipping {file_name} as it does not contain 'rank' in the file name.\")\n",
    "        return\n",
    "\n",
    "    # Change if different separator is used for distinguishing protein_1 and protein_2\n",
    "    if file_name.count(\"___\") == 1:\n",
    "        protein_1 = file_name.split(\"___\")[0]\n",
    "        protein_2_temp = file_name.split(\"___\")[1]\n",
    "        protein_2 = protein_2_temp.split(\"_unrelaxed\")[0]\n",
    "    else:\n",
    "        print(f\"Warning: Unexpected file naming convention for {file_name}. Skipping this file.\")\n",
    "        return None  # or some other default behavior\n",
    "\n",
    "    # Extract rank information\n",
    "    if \"_unrelaxed_rank_00\" in protein_2_temp:\n",
    "        rank_temp = protein_2_temp.split(\"_unrelaxed_rank_00\")[1]\n",
    "        rank = rank_temp.split(\"_alphafold2\")[0]\n",
    "    else:\n",
    "        rank = \"Not Available\"  # or any default value you prefer\n",
    "\n",
    "    if print_results:\n",
    "        print(\"Protein 1:\", protein_1)\n",
    "        print(\"Protein 2:\", protein_2)\n",
    "        print(\"Rank:\", rank)\n",
    "    \n",
    "    json_file = pdb_file_path.replace(\".pdb\", \".json\").replace(\"unrelaxed\", \"scores\")\n",
    "    structure = parser.get_structure(\"example\", pdb_file_path)\n",
    "\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            chain_id = chain.get_id()\n",
    "            chain_length = sum(1 for _ in chain.get_residues())\n",
    "            if chain_id == 'A':\n",
    "                protein_a_len = chain_length\n",
    "            # print(f\"Chain {chain_id} length : {chain_length}\")\n",
    "\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open(json_file, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    \n",
    "    plddt = statistics.mean(json_data[\"plddt\"])\n",
    "    ptm = json_data[\"ptm\"]\n",
    "    iptm = json_data[\"iptm\"]\n",
    "    pae = np.array(json_data['pae'])\n",
    "\n",
    "    # Calculate thresholded_pae\n",
    "    thresholded_pae = np.where(pae < pae_cutoff, 1, 0)\n",
    "\n",
    "    # Calculate the interaction amino acid numbers\n",
    "    local_interaction_protein_a = np.count_nonzero(thresholded_pae[:protein_a_len, :protein_a_len])\n",
    "    local_interaction_protein_b = np.count_nonzero(thresholded_pae[protein_a_len:, protein_a_len:])\n",
    "    local_interaction_interface_1 = np.count_nonzero(thresholded_pae[:protein_a_len, protein_a_len:])\n",
    "    local_interaction_interface_2 = np.count_nonzero(thresholded_pae[protein_a_len:, :protein_a_len])\n",
    "    local_interaction_interface_avg = (\n",
    "        local_interaction_interface_1 + local_interaction_interface_2\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Calculate average thresholded_pae for each region\n",
    "    average_thresholded_protein_a = thresholded_pae[:protein_a_len,:protein_a_len].mean() * 100\n",
    "    average_thresholded_protein_b = thresholded_pae[protein_a_len:,protein_a_len:].mean() * 100\n",
    "    average_thresholded_interaction1 = thresholded_pae[:protein_a_len,protein_a_len:].mean() * 100\n",
    "    average_thresholded_interaction2 = thresholded_pae[protein_a_len:,:protein_a_len].mean() * 100\n",
    "    average_thresholded_interaction_total = (average_thresholded_interaction1 + average_thresholded_interaction2) / 2\n",
    "    \n",
    "\n",
    "    pae_protein_a = np.mean( pae[:protein_a_len,:protein_a_len] )\n",
    "    pae_protein_b = np.mean( pae[protein_a_len:,protein_a_len:] )\n",
    "    pae_interaction1 = np.mean(pae[:protein_a_len,protein_a_len:])\n",
    "    pae_interaction2 = np.mean(pae[protein_a_len:,:protein_a_len])\n",
    "    pae_interaction_total = ( pae_interaction1 + pae_interaction2 ) / 2\n",
    "\n",
    "    # For pae_A\n",
    "    selected_values_protein_a = pae[:protein_a_len, :protein_a_len][thresholded_pae[:protein_a_len, :protein_a_len] == 1]\n",
    "    average_selected_protein_a = np.mean(selected_values_protein_a)\n",
    "\n",
    "    # For pae_B\n",
    "    selected_values_protein_b = pae[protein_a_len:, protein_a_len:][thresholded_pae[protein_a_len:, protein_a_len:] == 1]\n",
    "    average_selected_protein_b = np.mean(selected_values_protein_b)\n",
    "\n",
    "    # For pae_interaction1\n",
    "    selected_values_interaction1 = pae[:protein_a_len, protein_a_len:][thresholded_pae[:protein_a_len, protein_a_len:] == 1]\n",
    "    average_selected_interaction1 = np.mean(selected_values_interaction1) if selected_values_interaction1.size > 0 else pae_cutoff\n",
    "\n",
    "    # For pae_interaction2\n",
    "    selected_values_interaction2 = pae[protein_a_len:, :protein_a_len][thresholded_pae[protein_a_len:, :protein_a_len] == 1]\n",
    "    average_selected_interaction2 = np.mean(selected_values_interaction2) if selected_values_interaction2.size > 0 else pae_cutoff\n",
    "\n",
    "    # For pae_interaction_total\n",
    "    average_selected_interaction_total = (average_selected_interaction1 + average_selected_interaction2) / 2\n",
    "\n",
    "    if print_results:\n",
    "        # Print the total results\n",
    "        print(\"Total pae_A : {:.2f}\".format(pae_protein_a))\n",
    "        print(\"Total pae_B : {:.2f}\".format(pae_protein_b))\n",
    "        print(\"Total pae_i_1 : {:.2f}\".format(pae_interaction1))\n",
    "        print(\"Total pae_i_2 : {:.2f}\".format(pae_interaction2))\n",
    "        print(\"Total pae_i_avg : {:.2f}\".format(pae_interaction_total))\n",
    "\n",
    "        # Print the local results\n",
    "        print(\"Local pae_A : {:.2f}\".format(average_selected_protein_a))\n",
    "        print(\"Local pae_B : {:.2f}\".format(average_selected_protein_b))\n",
    "        print(\"Local pae_i_1 : {:.2f}\".format(average_selected_interaction1))\n",
    "        print(\"Local pae_i_2 : {:.2f}\".format(average_selected_interaction2))\n",
    "        print(\"Local pae_i_avg : {:.2f}\".format(average_selected_interaction_total))\n",
    "\n",
    "        # Print the >PAE-cutoff area\n",
    "        print(\"Local interaction area (Protein A):\", local_interaction_protein_a)\n",
    "        print(\"Local interaction area (Protein B):\", local_interaction_protein_b)\n",
    "        print(\"Local interaction area (Interaction 1):\", local_interaction_interface_1)\n",
    "        print(\"Local interaction area (Interaction 2):\", local_interaction_interface_2)\n",
    "        print(\"Total Interaction area (Interface):\", local_interaction_interface_avg)\n",
    "\n",
    "\n",
    "    # Transform the pae matrix\n",
    "    scaled_pae = reverse_and_scale_matrix(pae, pae_cutoff)\n",
    "\n",
    "    # For local interaction score for protein_a\n",
    "    selected_values_protein_a = scaled_pae[:protein_a_len, :protein_a_len][thresholded_pae[:protein_a_len, :protein_a_len] == 1]\n",
    "    average_selected_protein_a_score = np.mean(selected_values_protein_a)\n",
    "\n",
    "    # For local interaction score for protein_b\n",
    "    selected_values_protein_b = scaled_pae[protein_a_len:, protein_a_len:][thresholded_pae[protein_a_len:, protein_a_len:] == 1]\n",
    "    average_selected_protein_b_score = np.mean(selected_values_protein_b)\n",
    "\n",
    "    # For local interaction score1\n",
    "    selected_values_interaction1_score = scaled_pae[:protein_a_len, protein_a_len:][thresholded_pae[:protein_a_len, protein_a_len:] == 1]\n",
    "    average_selected_interaction1_score = np.mean(selected_values_interaction1_score) if selected_values_interaction1_score.size > 0 else 0\n",
    "\n",
    "    # For local interaction score2\n",
    "    selected_values_interaction2_score = scaled_pae[protein_a_len:, :protein_a_len][thresholded_pae[protein_a_len:, :protein_a_len] == 1]\n",
    "    average_selected_interaction2_score = np.mean(selected_values_interaction2_score) if selected_values_interaction2_score.size > 0 else 0\n",
    "\n",
    "    # For average local interaction score\n",
    "    average_selected_interaction_total_score = (average_selected_interaction1_score + average_selected_interaction2_score) / 2\n",
    "    \n",
    "    if print_results:\n",
    "        # Print the local interaction scores\n",
    "        print(\"Local Interaction Score_A : {:.3f}\".format(average_selected_protein_a_score))\n",
    "        print(\"Local Interaction Score_B : {:.3f}\".format(average_selected_protein_b_score))\n",
    "        print(\"Local Interaction Score_i_1 : {:.3f}\".format(average_selected_interaction1_score))\n",
    "        print(\"Local Interaction Score_i_2 : {:.3f}\".format(average_selected_interaction2_score))\n",
    "        print(\"Local Interaction Score_i_avg : {:.3f}\".format(average_selected_interaction_total_score))\n",
    "\n",
    "    COLUMNS_ORDER = [\n",
    "        'Protein_1', 'Protein_2', 'pLDDT', 'pTM', 'ipTM',\n",
    "        'Local_Score_A', 'Local_Score_B', 'Local_Score_i_1', 'Local_Score_i_2', 'Local_Score_i_avg',\n",
    "        'Local_Area_A', 'Local_Area_B', 'Local_Area_i_1', 'Local_Area_i_2', 'Local_Area_i_avg', \n",
    "        'Total_pae_A', 'Total_pae_B', 'Total_pae_i_1', 'Total_pae_i_2', 'Total_pae_i_avg',\n",
    "        'Local_pae_A', 'Local_pae_B', 'Local_pae_i_1', 'Local_pae_i_2', 'Local_pae_i_avg',\n",
    "        'Rank', 'saved folder', 'pdb', 'pae_file_name'\n",
    "    ]\n",
    "\n",
    "\n",
    "    return pd.Series({\n",
    "        'Protein_1': protein_1,\n",
    "        'Protein_2': protein_2,\n",
    "        'pLDDT': round(plddt, 2),\n",
    "        'pTM': ptm,\n",
    "        'ipTM': iptm,\n",
    "        'Total_pae_A': round(pae_protein_a, 2),\n",
    "        'Total_pae_B': round(pae_protein_b, 2),\n",
    "        'Total_pae_i_1': round(pae_interaction1, 2),\n",
    "        'Total_pae_i_2': round(pae_interaction2, 2),\n",
    "        'Total_pae_i_avg': round(pae_interaction_total, 2),\n",
    "        'Local_pae_A': round(average_selected_protein_a, 2),\n",
    "        'Local_pae_B': round(average_selected_protein_b, 2),\n",
    "        'Local_pae_i_1': round(average_selected_interaction1, 2),\n",
    "        'Local_pae_i_2': round(average_selected_interaction2, 2),\n",
    "        'Local_pae_i_avg': round(average_selected_interaction_total, 2),\n",
    "        'Local_Score_A': round(average_selected_protein_a_score, 3),\n",
    "        'Local_Score_B': round(average_selected_protein_b_score, 3),\n",
    "        'Local_Score_i_1': round(average_selected_interaction1_score, 3),\n",
    "        'Local_Score_i_2': round(average_selected_interaction2_score, 3),\n",
    "        'Local_Score_i_avg': round(average_selected_interaction_total_score, 3),\n",
    "        'Local_Area_A': local_interaction_protein_a,\n",
    "        'Local_Area_B': local_interaction_protein_b,\n",
    "        'Local_Area_i_1': local_interaction_interface_1,\n",
    "        'Local_Area_i_2': local_interaction_interface_2,\n",
    "        'Local_Area_i_avg': local_interaction_interface_avg,\n",
    "        'Rank': rank,\n",
    "        'saved folder': os.path.dirname(pdb_file_path),  # Gets the parent directory of the file path\n",
    "        'pdb': os.path.basename(pdb_file_path),  # Extracts just the base name of the pdb file\n",
    "        'pae_file_name': data_folder + '+' + protein_1 + '___' + protein_2 + '_pae.png'\n",
    "    }, name=file_name)[COLUMNS_ORDER]\n",
    "\n",
    "def reverse_and_scale_matrix(matrix: np.ndarray, pae_cutoff: float = 12.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Scale the values in the matrix such that:\n",
    "    0 becomes 1, pae_cutoff becomes 0, and values greater than pae_cutoff are also 0.\n",
    "    \n",
    "    Args:\n",
    "    - matrix (np.ndarray): Input numpy matrix.\n",
    "    - pae_cutoff (float): Threshold above which values become 0.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: Transformed matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Scale the values to [0, 1] for values between 0 and cutoff\n",
    "    scaled_matrix = (pae_cutoff - matrix) / pae_cutoff\n",
    "    scaled_matrix = np.clip(scaled_matrix, 0, 1)  # Ensures values are between 0 and 1\n",
    "    \n",
    "    return scaled_matrix\n",
    "\n",
    "\n",
    "\n",
    "def process_pdb_files(directory_path: str, processed_files=[], pae_cutoff: float = 12.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all .pdb files in the given directory and return the results as a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory_path: path to the directory containing the .pdb files.\n",
    "    - processed_files: a list of files that have already been processed.\n",
    "    - pae_cutoff: cutoff for the PAE calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    # List all files in the directory\n",
    "    all_files = os.listdir(directory_path)\n",
    "\n",
    "    # Filter for only .pdb files\n",
    "    pdb_files = [f for f in all_files if f.endswith(\".pdb\")]\n",
    "\n",
    "    # Create an empty dataframe to store results\n",
    "    df_results = pd.DataFrame()\n",
    "\n",
    "    # Apply the function to each PDB file\n",
    "    for pdb_file in pdb_files:\n",
    "        \n",
    "        # Check if this file has already been processed\n",
    "        if pdb_file in processed_files:\n",
    "#             print(f\"{pdb_file} already processed. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        pdb_file_path = os.path.join(directory_path, pdb_file)\n",
    "        print(\"\\nProcessing:\", pdb_file)\n",
    "        try:\n",
    "            results = calculate_pae(pdb_file_path, False, pae_cutoff)  # Assuming you've defined calculate_pae elsewhere\n",
    "            df_results = df_results.append(results, ignore_index=True)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File {pdb_file_path} not found. Skipping...\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "def main(base_path, saving_base_path, cutoff, folders_to_analyze, num_processes):\n",
    "    # Check and create saving_base_path if it doesn't exist\n",
    "    if not os.path.exists(saving_base_path):\n",
    "        os.makedirs(saving_base_path)\n",
    "    \n",
    "    for data_folder in folders_to_analyze:\n",
    "        directory_path = os.path.join(base_path, data_folder)\n",
    "        \n",
    "        if os.path.exists(directory_path):\n",
    "            output_filename = data_folder + \"_alphafold_analysis.csv\"\n",
    "            full_saving_path = os.path.join(saving_base_path, output_filename)\n",
    "            print(f\"Processing data from {directory_path}\")\n",
    "            print(f\"Saving to {full_saving_path}\")\n",
    "\n",
    "            # Check for existing processed files\n",
    "            if os.path.exists(full_saving_path):\n",
    "                try:\n",
    "                    existing_df = pd.read_csv(full_saving_path)\n",
    "                    processed_files = existing_df['pdb'].tolist() if 'pdb' in existing_df.columns else []\n",
    "                except EmptyDataError:\n",
    "                    # Handle the empty file situation\n",
    "                    print(f\"File {full_saving_path} is empty. Starting from scratch.\")\n",
    "                    existing_df = pd.DataFrame()\n",
    "                    processed_files = []\n",
    "            else:\n",
    "                existing_df = pd.DataFrame()\n",
    "                processed_files = []\n",
    "\n",
    "            # Process new files with multiprocessing\n",
    "            new_data = process_pdb_files_parallel(directory_path, processed_files, cutoff, num_processes)\n",
    "\n",
    "            # Combine old and new data only if there's new data\n",
    "            if not new_data.empty:\n",
    "                combined_df = pd.concat([existing_df, new_data])\n",
    "                \n",
    "                # Save the combined DataFrame\n",
    "                combined_df.to_csv(full_saving_path, index=False)\n",
    "                print(f\"Saved processed data to {full_saving_path}\")\n",
    "            else:\n",
    "                print(f\"No new data to append. CSV remains unchanged.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Directory {directory_path} does not exist! Skipping...\")\n",
    "\n",
    "\n",
    "\n",
    "def process_pdb_file(pdb_file, directory_path, processed_files, cutoff):\n",
    "    pdb_file_path = os.path.join(directory_path, pdb_file)\n",
    "    print(\"\\nProcessing:\", pdb_file)\n",
    "    try:\n",
    "        results = calculate_pae(pdb_file_path, False, cutoff)\n",
    "        return results\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {pdb_file_path} not found. Skipping...\")\n",
    "        return None\n",
    "\n",
    "def process_pdb_files_parallel(directory_path: str, processed_files=[], pae_cutoff: float = 12.0, num_processes=1) -> pd.DataFrame:\n",
    "    all_files = os.listdir(directory_path)\n",
    "    pdb_files = [f for f in all_files if f.endswith(\".pdb\") and f not in processed_files]\n",
    "\n",
    "    df_results = pd.DataFrame()\n",
    "    \n",
    "    with Pool(num_processes) as pool:\n",
    "        results = pool.starmap(calculate_pae, [(os.path.join(directory_path, f), False, pae_cutoff) for f in pdb_files])\n",
    "        for res in results:\n",
    "            if res is not None:\n",
    "                df_results = df_results.append(res, ignore_index=True)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "def get_subdirectories(base_path):\n",
    "    \"\"\"\n",
    "    Get a list of subdirectories in the given base path.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path: The path where to look for subdirectories.\n",
    "\n",
    "    Returns:\n",
    "    - List of subdirectories as strings.\n",
    "    \"\"\"\n",
    "    return [d.name for d in os.scandir(base_path) if d.is_dir()]\n",
    "\n",
    "\n",
    "def get_num_cpu_cores():\n",
    "    try:\n",
    "        return os.cpu_count() or 1\n",
    "    except AttributeError:\n",
    "        return multiprocessing.cpu_count() or 1\n",
    "\n",
    "num_processes = get_num_cpu_cores()\n",
    "print(f\"Number of available CPU cores: {num_processes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your base paths, cutoff value, and folder list\n",
    "cutoff = 12\n",
    "\n",
    "base_path = \"AFM output folder\"\n",
    "saving_base_path = \"/LIS_analysis/analysis_pae_\" + str(cutoff)  \n",
    "\n",
    "# Generate folders_to_analyze list\n",
    "folders_to_analyze = get_subdirectories(base_path)\n",
    "\n",
    "# # Call the main function with the folder list\n",
    "main(base_path, saving_base_path, cutoff, folders_to_analyze, num_processes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.core.base import DataError\n",
    "from pandas.errors import EmptyDataError\n",
    "import os\n",
    "\n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Process the given DataFrame to extract rank1 rows and compute average values.\n",
    "    Returns a new DataFrame with both rank1 and average information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop rows with any NaN values in the relevant columns\n",
    "    df = df.dropna(subset=['Local_Score_i_avg', 'Local_Area_i_avg', 'ipTM', 'pTM', 'pLDDT'])\n",
    "    \n",
    "    # Extract rank 1 rows and rename columns accordingly\n",
    "    rank1_rows = df[df['Rank'] == 1][['Protein_1', 'Protein_2', 'Local_Score_i_avg', 'Local_Area_i_avg', 'ipTM', 'pTM', 'pLDDT', 'pae_file_name']].copy()\n",
    "    rank1_rows.rename(columns={\n",
    "        'Local_Score_i_avg': 'Best LIS',\n",
    "        'Local_Area_i_avg': 'Best LIA',\n",
    "        'ipTM': 'Best ipTM',\n",
    "        'pTM': 'Best pTM',\n",
    "        'pLDDT': 'Best pLDDT'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Make sure to also keep 'pae_file_name' when you calculate the averages\n",
    "    average_values = df.groupby('pae_file_name', as_index=False)[['Local_Score_i_avg', 'Local_Area_i_avg', 'ipTM', 'pTM', 'pLDDT']].mean()\n",
    "    average_values.rename(columns={\n",
    "        'Local_Score_i_avg': 'Average LIS',\n",
    "        'Local_Area_i_avg': 'Average LIA',\n",
    "        'ipTM': 'Average ipTM',\n",
    "        'pTM': 'Average pTM',\n",
    "        'pLDDT': 'Average pLDDT'\n",
    "    }, inplace=True)\n",
    "\n",
    "    \n",
    "    # Merge rank 1 rows with the average values using 'pae_file_name' as the key\n",
    "    final_df = pd.merge(rank1_rows, average_values, on='pae_file_name', how='left')\n",
    "\n",
    "    # Define the columns of interest and their order\n",
    "    columns_of_interest = [\n",
    "        'Protein_1', 'Protein_2', \n",
    "        'Best LIS', 'Average LIS',\n",
    "        'Best LIA', 'Average LIA',\n",
    "        'Best ipTM', 'Average ipTM',\n",
    "        'Best pTM', 'Average pTM',\n",
    "        'Best pLDDT', 'Average pLDDT',\n",
    "        'pae_file_name'\n",
    "    ]\n",
    "\n",
    "    final_df = final_df[columns_of_interest]\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# Path to the specific folder where the original files are located\n",
    "folder_path = \"/LIS_analysis/analysis_pae_\" + str(cutoff)  \n",
    "\n",
    "# Path to the folder where you want to save the processed files\n",
    "saving_path = \"/LIS_analysis/analysis_pae_\" + str(cutoff)   + \"/averaged\"\n",
    "\n",
    "# Ensure the saving path directory exists, if not, create it\n",
    "if not os.path.exists(saving_path):\n",
    "    os.makedirs(saving_path)\n",
    "\n",
    "file_names = [f for f in os.listdir(folder_path) if f.endswith(\"alphafold_analysis.csv\")]\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    print(file_name)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if DataFrame is empty\n",
    "        if df.empty:\n",
    "            print(f\"File {file_name} is empty. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Process the DataFrame\n",
    "        processed_df = process_dataframe(df)\n",
    "\n",
    "        # Constructing the new file name\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        new_file_name = f\"{base_name}_processed.xlsx\"\n",
    "        new_file_path = os.path.join(saving_path, new_file_name)\n",
    "\n",
    "        # Save the processed DataFrame to a new file\n",
    "        processed_df.to_excel(new_file_path, index=False)\n",
    "        print(f\"Processed {file_name} and saved to {new_file_name}\")\n",
    "\n",
    "    except EmptyDataError:\n",
    "        print(f\"File {file_path} is empty and was skipped.\")\n",
    "    except DataError:\n",
    "        print(f\"Error processing {file_name}. Data types in the DataFrame are:\\n{df.dtypes}\\n\")\n",
    "\n",
    "print(\"Processing completed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
