{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from pathlib import Path \n",
    "from Bio.PDB import PDBParser\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "def create_structure_interaction_map(pdb_file, distance_cutoff=8):\n",
    "    \"\"\"\n",
    "    Create an interaction map for a protein structure from a PDB file.\n",
    "\n",
    "    Parameters:\n",
    "    pdb_file (str): Path to the PDB file.\n",
    "    distance_cutoff (float): Distance cutoff for interactions. Default is 8.\n",
    "\n",
    "    Returns:\n",
    "    interaction_map (np.array): Interaction map of the protein structure.\n",
    "    protein_a_len (int): Length of protein A.\n",
    "    protein_b_len (int): Length of protein B.\n",
    "    \"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    chains = list(structure.get_chains())\n",
    "    \n",
    "    if len(chains) < 2:\n",
    "        raise ValueError(\"Structure must contain at least two chains.\")\n",
    "    \n",
    "    coords_A, coords_B = [], []\n",
    "    for index, chain in enumerate(chains[:2]):\n",
    "        for residue in chain:\n",
    "            if 'CB' in residue or 'CA' in residue:\n",
    "                atom_name = 'CB' if 'CB' in residue else 'CA'\n",
    "                coords = residue[atom_name].get_coord()\n",
    "                if index == 0:\n",
    "                    coords_A.append(coords)\n",
    "                else:\n",
    "                    coords_B.append(coords)\n",
    "    \n",
    "    distances = distance.cdist(coords_A, coords_B)\n",
    "    interaction_map = (distances <= distance_cutoff).astype(int)\n",
    "    \n",
    "    protein_a_len = sum(1 for res in structure.get_residues() if res.get_parent().id == 'A')\n",
    "    protein_b_len = sum(1 for res in structure.get_residues() if res.get_parent().id == 'B')\n",
    "\n",
    "    return interaction_map, protein_a_len, protein_b_len\n",
    "\n",
    "def get_pLDDT_scores(pdb_file):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    # Use dictionary comprehension to directly create pLDDT_scores\n",
    "    pLDDT_scores = {(chain.id, i+1): residue['CA'].get_bfactor() \n",
    "                    for model in structure \n",
    "                    for chain in model \n",
    "                    for i, residue in enumerate(chain)}\n",
    "    \n",
    "    return pLDDT_scores\n",
    "\n",
    "def calculate_pLDDT_scores(interaction_map1, interaction_map2, pdb_file):\n",
    "    parser = PDBParser()\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    # total_bfactors = []\n",
    "    chain_bfactors = {}  # This dictionary will store the average B-factors for each chain\n",
    "\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            bfactors = [atom.get_bfactor() for atom in chain.get_atoms() if atom.get_bfactor() is not None]\n",
    "            if bfactors:\n",
    "                chain_bfactors[chain.id] = np.mean(bfactors)\n",
    "                # total_bfactors.extend(bfactors)\n",
    "\n",
    "    # Retrieve the values for chain 'A' and 'B', returning None if they are not found\n",
    "    whole_pLDDT_A = chain_bfactors.get('A', None)\n",
    "    whole_pLDDT_B = chain_bfactors.get('B', None)\n",
    "                \n",
    "    pLDDT_scores = get_pLDDT_scores(pdb_file)\n",
    "    interaction_maps = [interaction_map1, interaction_map2]\n",
    "    results = []\n",
    "    for interaction_map in interaction_maps:\n",
    "        if np.count_nonzero(interaction_map) == 0:\n",
    "            results.append((0, 0, 0))\n",
    "            continue\n",
    "\n",
    "        interaction_indices = np.where(interaction_map != 0)\n",
    "        positions_A = interaction_indices[0]\n",
    "        positions_B = interaction_indices[1]\n",
    "\n",
    "        unique_positions_A = np.unique(positions_A)\n",
    "        unique_positions_B = np.unique(positions_B)\n",
    "\n",
    "        # Initialize default values\n",
    "        pLDDT_A = []\n",
    "        average_pLDDT_A = 0\n",
    "        if unique_positions_A.size > 0:\n",
    "            pLDDT_A = [pLDDT_scores.get(('A', pos), 0) for pos in unique_positions_A]\n",
    "            average_pLDDT_A = np.mean(pLDDT_A) if pLDDT_A else 0\n",
    "\n",
    "        pLDDT_B = []\n",
    "        average_pLDDT_B = 0\n",
    "        if unique_positions_B.size > 0:\n",
    "            pLDDT_B = [pLDDT_scores.get(('B', pos), 0) for pos in unique_positions_B]\n",
    "            average_pLDDT_B = np.mean(pLDDT_B) if pLDDT_B else 0\n",
    "\n",
    "        # Calculate average pLDDT score in the interface\n",
    "        pLDDT_interface = pLDDT_A + pLDDT_B\n",
    "        average_pLDDT_interface = np.mean(pLDDT_interface) if pLDDT_interface else 0\n",
    "\n",
    "        results.append((average_pLDDT_A, average_pLDDT_B, average_pLDDT_interface))\n",
    "\n",
    "    return results, whole_pLDDT_A, whole_pLDDT_B\n",
    "\n",
    "def load_pae_data(pae_file):\n",
    "    try:\n",
    "        with open(pae_file, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Check if all required keys are present\n",
    "        required_keys = {\"plddt\", \"ptm\", \"iptm\", \"pae\"}\n",
    "        if not required_keys <= json_data.keys():\n",
    "            print(f\"Missing data in PAE file {pae_file}.\")\n",
    "            return None\n",
    "\n",
    "        plddt = statistics.mean(json_data[\"plddt\"])\n",
    "        ptm = json_data[\"ptm\"]\n",
    "        iptm = json_data[\"iptm\"]\n",
    "        pae = np.array(json_data['pae'])\n",
    "        return plddt, ptm, iptm, pae\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PAE data from {pae_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_interaction_areas(pae, protein_a_len, pae_cutoff, interaction_map):\n",
    "    thresholded_pae = np.where(pae < pae_cutoff, 1, 0)\n",
    "    \n",
    "    # Extract interaction areas for AB and BA\n",
    "    area_ab = thresholded_pae[:protein_a_len, protein_a_len:]\n",
    "    area_ba = thresholded_pae[protein_a_len:, :protein_a_len]\n",
    "    \n",
    "    # Pad interaction_map to match the shape of area_ab if necessary\n",
    "    pad_rows_im = area_ab.shape[0] - interaction_map.shape[0]\n",
    "    pad_cols_im = area_ab.shape[1] - interaction_map.shape[1]\n",
    "    padded_interaction_map = np.pad(interaction_map, ((0, pad_rows_im), (0, pad_cols_im)), mode='constant', constant_values=0)\n",
    "    \n",
    "    # Pad transpose of interaction_map to match the shape of area_ba if necessary\n",
    "    pad_rows_imt = area_ba.shape[0] - interaction_map.T.shape[0]\n",
    "    pad_cols_imt = area_ba.shape[1] - interaction_map.T.shape[1]\n",
    "    padded_interaction_map_t = np.pad(interaction_map.T, ((0, pad_rows_imt), (0, pad_cols_imt)), mode='constant', constant_values=0)\n",
    "    \n",
    "    # Calculate contacts where both PAE and interaction_map indicate interaction\n",
    "    contact_ab = np.where((area_ab > 0) & (padded_interaction_map > 0), 1, 0)\n",
    "    contact_ba = np.where((area_ba > 0) & (padded_interaction_map_t > 0), 1, 0)\n",
    "    \n",
    "    return area_ab, area_ba, contact_ab, contact_ba\n",
    "\n",
    "\n",
    "def comparing_interaction_area(area_ab, area_ba, contact_ab, contact_ba):\n",
    "    local_interaction_area_ab = np.sum(area_ab)\n",
    "    local_interaction_area_ba = np.sum(area_ba)\n",
    "    local_interaction_area_both = local_interaction_area_ab + local_interaction_area_ba\n",
    "    contact_local_interaction_area_ab = np.sum(contact_ab)\n",
    "    contact_local_interaction_area_ba = np.sum(contact_ba)\n",
    "    contact_local_interaction_area = contact_local_interaction_area_ab + contact_local_interaction_area_ba\n",
    "    return local_interaction_area_ab, local_interaction_area_ba, local_interaction_area_both, contact_local_interaction_area_ab, contact_local_interaction_area_ba, contact_local_interaction_area\n",
    "\n",
    "def calculate_pae_values(pae, area_ab, area_ba, contact_ab, contact_ba, protein_a_len, pae_cutoff):\n",
    "    domain_pae_ab = pae[:protein_a_len, protein_a_len:][area_ab==1]\n",
    "    domain_pae_ba = pae[protein_a_len:, :protein_a_len][area_ba==1]\n",
    "    contact_pae_ab = pae[:protein_a_len, protein_a_len:][contact_ab==1]\n",
    "    contact_pae_ba = pae[protein_a_len:, :protein_a_len][contact_ba==1]\n",
    "    mean_domain_pae_ab = np.mean(domain_pae_ab) if domain_pae_ab.size > 0 else pae_cutoff\n",
    "    mean_domain_pae_ba = np.mean(domain_pae_ba) if domain_pae_ba.size > 0 else pae_cutoff\n",
    "    mean_contact_pae_ab = np.mean(contact_pae_ab) if contact_pae_ab.size > 0 else pae_cutoff \n",
    "    mean_contact_pae_ba = np.mean(contact_pae_ba) if contact_pae_ba.size > 0 else pae_cutoff\n",
    "    return mean_domain_pae_ab, mean_domain_pae_ba, mean_contact_pae_ab, mean_contact_pae_ba\n",
    "\n",
    "def calculate_inversed_pae(mean_domain_pae_ab, mean_domain_pae_ba, mean_contact_pae_ab, mean_contact_pae_ba, pae_cutoff):\n",
    "    inversed_mean_domain_pae_ab = 1 - mean_domain_pae_ab / pae_cutoff\n",
    "    inversed_mean_domain_pae_ba = 1 - mean_domain_pae_ba / pae_cutoff\n",
    "    inversed_mean_domain_pae_both = (inversed_mean_domain_pae_ab + inversed_mean_domain_pae_ba)/2\n",
    "    inversed_mean_contact_pae_ab = 1 - mean_contact_pae_ab / pae_cutoff\n",
    "    inversed_mean_contact_pae_ba = 1 - mean_contact_pae_ba / pae_cutoff\n",
    "    inversed_mean_contact_pae_both = (inversed_mean_contact_pae_ab + inversed_mean_contact_pae_ba) / 2\n",
    "    return inversed_mean_domain_pae_ab, inversed_mean_domain_pae_ba, inversed_mean_domain_pae_both, inversed_mean_contact_pae_ab, inversed_mean_contact_pae_ba, inversed_mean_contact_pae_both\n",
    "\n",
    "def calculate_residue_indices(area_ab, area_ba, contact_ab, contact_ba):\n",
    "    area_sum = area_ab + area_ba.transpose()\n",
    "    contact_sum = contact_ab + contact_ba.transpose()\n",
    "    area_residue_indices = np.nonzero(area_sum)\n",
    "    contact_residue_indices = np.nonzero(contact_sum)\n",
    "    return area_residue_indices, contact_residue_indices, contact_sum, area_sum\n",
    "\n",
    "def calculate_residue_counts(area_residue_indices, contact_residue_indices):\n",
    "    unique_area_residue_indices_A = np.unique(area_residue_indices[0]) + 1 # fixed on 2024/10/24\n",
    "    unique_area_residue_indices_B = np.unique(area_residue_indices[1]) + 1\n",
    "    area_residue_A = len(unique_area_residue_indices_A)\n",
    "    area_residue_B = len(unique_area_residue_indices_B)\n",
    "    area_residue_both = area_residue_A + area_residue_B\n",
    "\n",
    "    unique_contact_residue_indices_A = np.unique(contact_residue_indices[0]) + 1\n",
    "    unique_contact_residue_indices_B = np.unique(contact_residue_indices[1]) + 1\n",
    "    contact_residue_A = len(unique_contact_residue_indices_A)\n",
    "    contact_residue_B = len(unique_contact_residue_indices_B)\n",
    "    contact_residue_both = contact_residue_A + contact_residue_B\n",
    "\n",
    "    return area_residue_A, area_residue_B, area_residue_both, contact_residue_A, contact_residue_B, contact_residue_both, unique_area_residue_indices_A, unique_area_residue_indices_B,unique_contact_residue_indices_A, unique_contact_residue_indices_B\n",
    "\n",
    "def process_interaction_results(pdb_file, pae_file, pae_cutoff=12, distance_cutoff=8, print_results=False):\n",
    "    \"\"\"\n",
    "    Process the interaction results.\n",
    "\n",
    "    Parameters:\n",
    "    pdb_file (str): Path to the PDB file.\n",
    "    pae_file (str): Path to the PAE file.\n",
    "    pae_cutoff (float): PAE cutoff value.\n",
    "    distance_cutoff (float): Distance cutoff for interactions.\n",
    "    print_results (bool): Flag to indicate whether to print the results.\n",
    "    \n",
    "    Returns:\n",
    "    interaction_data (pd.Series): Series containing the interaction results.\n",
    "    \"\"\"\n",
    "    ## 2024/05/25 added ##\n",
    "    # Check if both PDB and PAE files are available\n",
    "    if not os.path.isfile(pdb_file) or not os.path.isfile(pae_file):\n",
    "        print(f\"Warning: One or both files are missing: PDB ({pdb_file}), PAE ({pae_file}). Skipping this analysis.\")\n",
    "        return None\n",
    "    ## 2024/05/25 added ##\n",
    "    \n",
    "    # Extract protein names and rank from PDB file name\n",
    "    file_name = os.path.basename(pdb_file)\n",
    "    if file_name.count(\"___\") == 1:\n",
    "        protein_1 = file_name.split(\"___\")[0]\n",
    "        protein_2_temp = file_name.split(\"___\")[1]\n",
    "        protein_2 = protein_2_temp.split(\"_unrelaxed\")[0]\n",
    "    else:\n",
    "        print(f\"Warning: Unexpected file naming convention for {file_name}. Skipping this file.\")\n",
    "        return None  # or some other default behavior\n",
    "\n",
    "    # Extract rank information\n",
    "    if \"_unrelaxed_rank_00\" in protein_2_temp:\n",
    "        rank_temp = protein_2_temp.split(\"_unrelaxed_rank_00\")[1]\n",
    "        rank = rank_temp.split(\"_alphafold2\")[0]\n",
    "    else:\n",
    "        print(f\"Warning: Unexpected file naming convention for {file_name}. Skipping this file.\")\n",
    "        return None  # or some other default behavior    \n",
    "    \n",
    "    # Load PAE data\n",
    "    plddt, ptm, iptm, pae = load_pae_data(pae_file)\n",
    "    pdb_base_folder = os.path.basename(os.path.dirname(pdb_file))\n",
    "    pdb_file_name = os.path.basename(pdb_file)\n",
    "    pae_file_name = os.path.basename(pae_file)\n",
    "\n",
    "    if pae is not None:\n",
    "        interaction_map, protein_a_len, protein_b_len = create_structure_interaction_map(pdb_file, distance_cutoff)\n",
    "        area_ab, area_ba, contact_ab, contact_ba = calculate_interaction_areas(pae, protein_a_len, pae_cutoff, interaction_map)\n",
    "        local_interaction_area_ab, local_interaction_area_ba, local_interaction_area_both, contact_local_interaction_area_ab, contact_local_interaction_area_ba, contact_local_interaction_area = comparing_interaction_area(area_ab, area_ba, contact_ab, contact_ba)\n",
    "        mean_domain_pae_ab, mean_domain_pae_ba, mean_contact_pae_ab, mean_contact_pae_ba = calculate_pae_values(pae, area_ab, area_ba, contact_ab, contact_ba, protein_a_len, pae_cutoff)\n",
    "        inversed_mean_domain_pae_ab, inversed_mean_domain_pae_ba, inversed_mean_domain_pae_both, inversed_mean_contact_pae_ab, inversed_mean_contact_pae_ba, inversed_mean_contact_pae_both = calculate_inversed_pae(mean_domain_pae_ab, mean_domain_pae_ba, mean_contact_pae_ab, mean_contact_pae_ba, pae_cutoff)\n",
    "        area_residue_indices, contact_residue_indices, contact_sum, area_sum = calculate_residue_indices(area_ab, area_ba, contact_ab, contact_ba)\n",
    "        area_residue_A, area_residue_B, area_residue_both, contact_residue_A, contact_residue_B, contact_residue_both, unique_area_residue_indices_A, unique_area_residue_indices_B,unique_contact_residue_indices_A, unique_contact_residue_indices_B = calculate_residue_counts(area_residue_indices, contact_residue_indices)\n",
    "        results, pLDDT_A, pLDDT_B = calculate_pLDDT_scores(contact_sum, area_sum, pdb_file)\n",
    "        contact_pLDDT_A, contact_pLDDT_B, contact_pLDDT_interface = results[0]\n",
    "        domain_pLDDT_A, domain_pLDDT_B, domain_pLDDT_interface = results[1]   \n",
    "\n",
    "        # Create a series containing the interaction results\n",
    "        interaction_data = pd.Series({\n",
    "            'Protein_1': protein_1,\n",
    "            'Protein_2': protein_2,\n",
    "            'Rank': rank,\n",
    "            'integrated Local Interaction Score (Interface)': np.sqrt(inversed_mean_domain_pae_both*inversed_mean_contact_pae_both),\n",
    "            'Local Interaction Score (AB)': inversed_mean_domain_pae_ab,\n",
    "            'Local Interaction Score (BA)': inversed_mean_domain_pae_ba,\n",
    "            'Local Interaction Score (Interface)': inversed_mean_domain_pae_both,\n",
    "            'Local Interaction Area (AB)': local_interaction_area_ab,\n",
    "            'Local Interaction Area (BA)': local_interaction_area_ba,\n",
    "            'Local Interaction Area (Interface)': local_interaction_area_both,\n",
    "            'Local Interaction Residue (A)': area_residue_A,\n",
    "            'Local Interaction Residue (B)': area_residue_B,\n",
    "            'Local Interaction Residue (Sum)': area_residue_both,\n",
    "            'Local Interaction pLDDT (A)': domain_pLDDT_A,\n",
    "            'Local Interaction pLDDT (B)': domain_pLDDT_B,\n",
    "            'Local Interaction pLDDT (Interface)': domain_pLDDT_interface,\n",
    "            'Contact Local Interaction Score (AB)': inversed_mean_contact_pae_ab,\n",
    "            'Contact Local Interaction Score (BA)': inversed_mean_contact_pae_ba,\n",
    "            'Contact Local Interaction Score (Interface)': inversed_mean_contact_pae_both,\n",
    "            'Contact Local Interaction Area (AB)': contact_local_interaction_area_ab,\n",
    "            'Contact Local Interaction Area (BA)': contact_local_interaction_area_ba,\n",
    "            'Contact Local Interaction Area (Interface)': contact_local_interaction_area,\n",
    "            'Contact Local Interaction Residue (A)': contact_residue_A,\n",
    "            'Contact Local Interaction Residue (B)': contact_residue_B,\n",
    "            'Contact Local Interaction Residue (Sum)': contact_residue_both,\n",
    "            'Contact Local Interaction pLDDT (A)': contact_pLDDT_A,\n",
    "            'Contact Local Interaction pLDDT (B)': contact_pLDDT_B,\n",
    "            'Contact Local Interaction pLDDT (Interface)': contact_pLDDT_interface,\n",
    "            'Local Interaction Residue Indice A': unique_area_residue_indices_A,\n",
    "            'Local Interaction Residue Indice B': unique_area_residue_indices_B,\n",
    "            'Contact Local Interaction Residue Indice A': unique_contact_residue_indices_A,\n",
    "            'Contact Local Interaction Residue Indice B': unique_contact_residue_indices_B,\n",
    "            'pLDDT': plddt,\n",
    "            'pTM': ptm,\n",
    "            'ipTM': iptm,\n",
    "            'confidence': 0.8*iptm + 0.2*ptm,\n",
    "            'Protein A Length': int(protein_a_len),\n",
    "            'Protein B Length': int(protein_b_len),\n",
    "            'pLDDT_A': pLDDT_A,\n",
    "            'pLDDT_B': pLDDT_B,\n",
    "            'pdb_file': pdb_file_name,\n",
    "            'pae_json': pae_file_name,\n",
    "            'pae_plot': f'{pdb_base_folder}+{protein_1}___{protein_2}_pae.png',\n",
    "\n",
    "        })\n",
    "\n",
    "        if print_results:\n",
    "            print(interaction_data)\n",
    "\n",
    "    return interaction_data, contact_sum, area_sum\n",
    "\n",
    "\n",
    "def get_subdirectories(base_path):\n",
    "    return [d.name for d in os.scandir(base_path) if d.is_dir()]\n",
    "\n",
    "def pdb_file_info_extract(folder_path, pdb_file):\n",
    "    pdb_file = os.path.join(folder_path, pdb_file)\n",
    "    pae_file = pdb_file.replace(\".pdb\", \".json\").replace(\"unrelaxed\", \"scores\")\n",
    "    \n",
    "    file_name = os.path.basename(pdb_file)\n",
    "    if file_name.count(\"___\") == 1:\n",
    "        protein_1 = file_name.split(\"___\")[0]\n",
    "        protein_2_temp = file_name.split(\"___\")[1]\n",
    "        protein_2 = protein_2_temp.split(\"_unrelaxed\")[0]\n",
    "    else:\n",
    "        print(f\"Warning: Unexpected file naming convention for {file_name}. Skipping this file.\")\n",
    "        return None  # or some other default behavior\n",
    "\n",
    "    # Extract rank information\n",
    "    if \"_unrelaxed_rank_00\" in protein_2_temp:\n",
    "        rank_temp = protein_2_temp.split(\"_unrelaxed_rank_00\")[1]\n",
    "        rank = rank_temp.split(\"_alphafold2\")[0]\n",
    "    else:\n",
    "        rank = \"Not Available\"  # or any default value you prefer\n",
    "\n",
    "    return pdb_file, pae_file, protein_1, protein_2, rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def process_interaction_results_if_needed(pdb_file, pae_file, output_path, pae_cutoff=12, distance_cutoff=8, print_results=False):\n",
    "\n",
    "        ## 2024/05/25 added ##\n",
    "    # Check if both PDB and PAE files are available\n",
    "    if not os.path.isfile(pdb_file) or not os.path.isfile(pae_file):\n",
    "        print(f\"Warning: One or both files are missing: PDB ({pdb_file}), PAE ({pae_file}). Skipping this analysis.\")\n",
    "        return None\n",
    "    ## 2024/05/25 added ##\n",
    "    \n",
    "    file_name = os.path.basename(pdb_file)\n",
    "    if file_name.count(\"___\") == 1:\n",
    "        protein_1 = file_name.split(\"___\")[0]\n",
    "        protein_2_temp = file_name.split(\"___\")[1]\n",
    "        protein_2 = protein_2_temp.split(\"_unrelaxed\")[0]\n",
    "    else:\n",
    "        print(f\"Warning: Unexpected file naming convention for {file_name}. Skipping this file.\")\n",
    "        return None  # or some other default behavior\n",
    "\n",
    "    # Extract rank information\n",
    "    if \"_unrelaxed_rank_00\" in protein_2_temp:\n",
    "        rank_temp = protein_2_temp.split(\"_unrelaxed_rank_00\")[1]\n",
    "        rank = int(rank_temp.split(\"_alphafold2\")[0])\n",
    "    else:\n",
    "        rank = \"Not Available\"  # or any default value you prefer\n",
    "        \n",
    "\n",
    "    output_file = os.path.join(output_path, f\"{protein_1}___{protein_2}_rank_00{rank}_lis.tsv\")\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "#         print(f\"Skipping analysis for {file_name} as output already exists.\")\n",
    "        return None  # Return None if file exists\n",
    "\n",
    "    interaction_data, contact_sum, area_sum = process_interaction_results(pdb_file, pae_file, pae_cutoff, distance_cutoff, print_results)\n",
    "    interaction_data_df = interaction_data.to_frame().T\n",
    "    interaction_data_df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "    \n",
    "#     interaction_data.to_csv(output_file, sep='\\t', index=False)\n",
    "#     interaction_data_transposed = interaction_data.transpose()\n",
    "#     interaction_data_transposed.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "    return #f\"Processed {file_name}\"\n",
    "\n",
    "def process_files_in_folder(folder, base_path, output_base, pae_cutoff, distance_cutoff):\n",
    "    output_path = Path(output_base).joinpath(folder)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    folder_path = Path(base_path).joinpath(folder)\n",
    "    \n",
    "    # List comprehension to filter only files that contain '_rank_' in the filename\n",
    "    pdb_files = [pdb_file for pdb_file in folder_path.rglob(\"*.pdb\") if '_rank_00' in str(pdb_file)]\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_interaction_results_if_needed, str(pdb_file), str(pdb_file).replace(\".pdb\", \".json\").replace(\"unrelaxed\", \"scores\"), str(output_path), pae_cutoff, distance_cutoff) for pdb_file in pdb_files]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                print(result)\n",
    "\n",
    "                \n",
    "## 2024/05/30 update\n",
    "def combine_tsv_files_incrementally(input_directory, output_directory):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    group_name = os.path.basename(input_directory)\n",
    "    output_filename = os.path.join(output_directory, f\"{group_name}_lis_all_ranks.csv\")\n",
    "\n",
    "    # Check if a combined file already exists in the output directory\n",
    "    existing_data = None\n",
    "    if os.path.exists(output_filename):\n",
    "        existing_data = pd.read_csv(output_filename)  # defaults to comma as separator\n",
    "        # Check if 'source_file' column exists, if not, create it\n",
    "        if 'source_file' not in existing_data.columns:\n",
    "            existing_data['source_file'] = pd.NA  # Initialize with missing values\n",
    "        existing_files = set(existing_data['source_file'].dropna().unique())\n",
    "    else:\n",
    "        existing_files = set()\n",
    "\n",
    "    # Dataframe to store new data\n",
    "    new_data = []\n",
    "\n",
    "    # Iterate over all tsv files in the input directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.tsv') and filename not in existing_files:\n",
    "            # Assuming the original TSV files are also separated by tabs\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(input_directory, filename), sep='\\t')\n",
    "                df['source_file'] = filename  # Add a column to track the source file\n",
    "                new_data.append(df)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"Skipping empty file: {filename}\")\n",
    "\n",
    "    # Combine new data with existing data if there is any new data\n",
    "    if new_data:\n",
    "        new_combined_df = pd.concat(new_data, ignore_index=True)\n",
    "        if existing_data is not None:\n",
    "            new_combined_df = pd.concat([existing_data, new_combined_df], ignore_index=True)\n",
    "        # Save the updated combined data to the output directory without specifying sep\n",
    "        new_combined_df.to_csv(output_filename, index=False)\n",
    "    else:\n",
    "        print(f\"No new data to combine in {input_directory}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColabFold LIS Analysis Script\n",
    "This analyzes ColabFold outputs stored in {base_path} in the server.\n",
    "\n",
    "It processes multiple result folders, computes LIS metrics, and aggregates results.\n",
    "\n",
    "\n",
    "### Expected Directory Structure:\n",
    "    colabfold_output/\n",
    "    ├── folder_1/  (Contains .pdb and .json prediction results)\n",
    "    ├── folder_2/\n",
    "    ├── ...\n",
    "\n",
    "### Output:\n",
    "- LIS analysis results saved in {output_base} folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_subdirectories(base_path):\n",
    "    subdirectories = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    subdirectories_with_times = [(d, os.path.getmtime(os.path.join(base_path, d))) for d in subdirectories]\n",
    "    \n",
    "    # Debug: Print the directories with their raw and local time modification times\n",
    "#     print(\"Directories with their modification times:\")\n",
    "#     for d, t in subdirectories_with_times:\n",
    "#         print(f\"Directory: {d}, Last Modified Raw: {t}, Last Modified Local: {datetime.fromtimestamp(t)}\")\n",
    "    \n",
    "    # Sort subdirectories by modification time in descending order\n",
    "    sorted_subdirectories = sorted(subdirectories_with_times, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Debug: Print the sorted directories\n",
    "#     print(\"Sorted directories by modification time:\")\n",
    "#     for d, t in sorted_subdirectories:\n",
    "#         print(f\"Directory: {d}, Last Modified: {datetime.fromtimestamp(t)}\")\n",
    "    \n",
    "    # Ensure sorting is correct\n",
    "    assert all(sorted_subdirectories[i][1] >= sorted_subdirectories[i+1][1] for i in range(len(sorted_subdirectories) - 1)), \"Sorting failed!\"\n",
    "\n",
    "    return [d[0] for d in sorted_subdirectories]\n",
    "\n",
    "\n",
    "# Parameters\n",
    "pae_cutoff = 12\n",
    "distance_cutoff=8\n",
    "\n",
    "base_path = \"colabfold_output\" # change this\n",
    "output_base = f\"analysis_pae_{pae_cutoff}\" \n",
    "saving_all_rank_folder = f\"{output_base}_all_ranks\"\n",
    "contact_lis_folder = output_base\n",
    "\n",
    "# Generate folders_to_analyze list\n",
    "folders_to_analyze = get_subdirectories(base_path)\n",
    "\n",
    "# Main processing loop\n",
    "for iteration in range(1):  \n",
    "    current_iteration_folders = set()\n",
    "\n",
    "    for folder in folders_to_analyze:\n",
    "        if folder not in current_iteration_folders:\n",
    "            print(f\"Processing {folder}\")\n",
    "            process_files_in_folder(folder, base_path, output_base, pae_cutoff=pae_cutoff, distance_cutoff=distance_cutoff) \n",
    "            print(f\"Finished processing and combining results for {folder}\")\n",
    "            data_folder = os.path.join(contact_lis_folder, folder)\n",
    "            combine_tsv_files_incrementally(data_folder, saving_all_rank_folder)\n",
    "            current_iteration_folders.add(folder)\n",
    "\n",
    "    print(f\"Iteration {iteration + 1} complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIS Analysis File Search & Processing Script\n",
    "\n",
    "The searches for LIS analysis files in a given directory, \n",
    "matches them against specific search terms, and processes them for further analysis.\n",
    "\n",
    "### Expected Directory Structure:\n",
    "    analysis_pae_12_all_ranks/\n",
    "    ├── screen_1_lis_all_ranks.csv\n",
    "    ├── screen_2_lis_all_ranks.csv\n",
    "    ├── screen_3_lis_all_ranks.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import fnmatch\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "def find_files(directory, search_terms):\n",
    "    # Initialize an empty list to store the matching files\n",
    "    file_names = []\n",
    "    # Iterate over all search terms\n",
    "    for term in search_terms:\n",
    "        # Use glob to find all files that contain the search term\n",
    "        matching_files = glob.glob(f'{directory}/*{term}*.csv', recursive=True)\n",
    "        for file in matching_files:\n",
    "            file_name = os.path.basename(file)\n",
    "            file_names.append(file_name)\n",
    "    return file_names\n",
    "\n",
    "threshold_file = \"threshold_file\" # change this\n",
    "\n",
    "# Define the directory where the search starts\n",
    "directory = 'analysis_pae_12_all_ranks' # change this\n",
    "\n",
    "search_terms = [\n",
    "\"screen\",\n",
    "]\n",
    "\n",
    "searched_files = find_files(directory, search_terms)\n",
    "print(searched_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIS Analysis Data Processing\n",
    "\n",
    "This processes LIS analysis files by:\n",
    "\n",
    "- Searching for relevant .csv files based on search terms  \n",
    "- Renaming columns  \n",
    "- Calculating integrated LIS (iLIS) using the formula:  \n",
    "\n",
    "  $iLIS = \\sqrt{LIS \\times cLIS}$  \n",
    "\n",
    "- Extracting and applying thresholds from an external Excel threshold file  \n",
    "- Calculating a score based on the defined threshold preferences  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for searched_file in searched_files:\n",
    "    df = pd.read_csv(f'{directory}/{searched_file}')\n",
    "    dfs.append(df)  \n",
    "\n",
    "df_combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "column_mapping = {\n",
    "    'Protein_1': 'Protein_1',\n",
    "    'Protein_2': 'Protein_2',\n",
    "    'Rank': 'Rank',\n",
    "    'integrated Local Interaction Score (Interface)': 'iLIS',\n",
    "    'Local Interaction Score (AB)': 'LIS_AB',\n",
    "    'Local Interaction Score (BA)': 'LIS_BA',\n",
    "    'Local Interaction Score (Interface)': 'LIS',\n",
    "    'Local Interaction Area (AB)': 'LIA_AB',\n",
    "    'Local Interaction Area (BA)': 'LIA_BA',\n",
    "    'Local Interaction Area (Interface)': 'LIA',\n",
    "    'Local Interaction Residue (A)': 'LIR_A',\n",
    "    'Local Interaction Residue (B)': 'LIR_B',\n",
    "    'Local Interaction Residue (Sum)': 'LIR',\n",
    "    'Local Interaction pLDDT (A)': 'LIpLDDT_A',\n",
    "    'Local Interaction pLDDT (B)': 'LIpLDDT_B',\n",
    "    'Local Interaction pLDDT (Interface)': 'LIpLDDT',\n",
    "    'Contact Local Interaction Score (AB)': 'cLIS_AB',\n",
    "    'Contact Local Interaction Score (BA)': 'cLIS_BA',\n",
    "    'Contact Local Interaction Score (Interface)': 'cLIS',\n",
    "    'Contact Local Interaction Area (AB)': 'cLIA_AB',\n",
    "    'Contact Local Interaction Area (BA)': 'cLIA_BA',\n",
    "    'Contact Local Interaction Area (Interface)': 'cLIA',\n",
    "    'Contact Local Interaction Residue (A)': 'cLIR_A',\n",
    "    'Contact Local Interaction Residue (B)': 'cLIR_B',\n",
    "    'Contact Local Interaction Residue (Sum)': 'cLIR',\n",
    "    'Contact Local Interaction pLDDT (A)': 'cLIpLDDT_A',\n",
    "    'Contact Local Interaction pLDDT (B)': 'cLIpLDDT_B',\n",
    "    'Contact Local Interaction pLDDT (Interface)': 'cLIpLDDT',\n",
    "    'Local Interaction Residue Indice A': 'LIR_indice_A',\n",
    "    'Local Interaction Residue Indice B': 'LIR_indice_B',\n",
    "    'Contact Local Interaction Residue Indice A': 'cLIR_indice_A',\n",
    "    'Contact Local Interaction Residue Indice B': 'cLIR_indice_B',\n",
    "    'pLDDT': 'pLDDT',\n",
    "    'pTM': 'pTM',\n",
    "    'ipTM': 'ipTM',\n",
    "    'Protein A Length': 'Protein_Len_A',\n",
    "    'Protein B Length': 'Protein_Len_B',\n",
    "    'pdb_file': 'pdb_file',\n",
    "    'pae_json': 'pae_json',\n",
    "    'pae_plot': 'pae_plot',\n",
    "    'confidence': 'Confidence'\n",
    "}\n",
    "\n",
    "df_combined = df_combined.rename(columns=column_mapping)\n",
    "\n",
    "df_combined['folder'] = df_combined['pae_plot'].str.split('+').str[0]\n",
    "df_combined['Total length'] = df_combined['Protein_Len_A'] + df_combined['Protein_Len_B']\n",
    "\n",
    "# Assuming df_final is your DataFrame and already has 'LIS' and 'cLIS'\n",
    "df_combined['LIS*cLIS'] = df_combined[\"LIS\"] * df_combined[\"cLIS\"]\n",
    "\n",
    "# Apply min-max scaling using constants\n",
    "df_combined['iLIS'] = np.sqrt(df_combined['LIS*cLIS'])\n",
    "df_combined = df_combined.drop(columns=['LIS*cLIS'])\n",
    "\n",
    "df_threshold_yfh = pd.read_excel(threshold_file)\n",
    "\n",
    "# # Ensure the gene IDs and proteins are treated as strings\n",
    "df_combined['Protein_1'] = df_combined['Protein_1'].astype(str)\n",
    "df_combined['Protein_2'] = df_combined['Protein_2'].astype(str)\n",
    "\n",
    "# Extract thresholds for each metric\n",
    "metrics = ['iLIS', 'LIS', 'cLIS', 'ipTM', 'Confidence'] \n",
    "thresholds = {}\n",
    "for metric in metrics:\n",
    "    metric_thresholds = df_threshold_yfh[(df_threshold_yfh['Group'] == 'Total') & (df_threshold_yfh['Column'] == metric)]\n",
    "    if not metric_thresholds.empty:\n",
    "        metric_thresholds = metric_thresholds.iloc[0]\n",
    "        thresholds[metric] = {\n",
    "            'Threshold@0.10': metric_thresholds['Threshold@0.10'],\n",
    "            'Threshold@0.05': metric_thresholds['Threshold@0.05'],\n",
    "            'Threshold@0.01': metric_thresholds['Threshold@0.01']\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Thresholds for metric '{metric}' not found. Please check the input data.\")\n",
    "\n",
    "# Change score depending on preference\n",
    "def calculate_score(row, thresholds):\n",
    "    score = 0\n",
    "    for metric in thresholds:\n",
    "        value = row[metric]\n",
    "        if value >= thresholds[metric]['Threshold@0.01']:\n",
    "            score += 1\n",
    "        elif value >= thresholds[metric]['Threshold@0.05']:\n",
    "            score += 1\n",
    "        elif value >= thresholds[metric]['Threshold@0.10']:\n",
    "            score += 1\n",
    "    return score\n",
    "\n",
    "\n",
    "# Apply the score calculation function to each row\n",
    "df_combined['Score'] = df_combined.apply(lambda row: calculate_score(row, thresholds), axis=1)\n",
    "df_combined = df_combined[~df_combined.duplicated(subset=['Rank', 'pae_plot'])]\n",
    " \n",
    "# Save the results to an Excel file\n",
    "df_plot = df_combined[['Protein_1', \n",
    "             'Protein_2',\n",
    "             'Rank',\n",
    "             'Score',\n",
    "             'iLIS', \n",
    "             'LIS', \n",
    "             'cLIS', \n",
    "             'ipTM', \n",
    "             'Confidence',\n",
    "             'pae_plot',\n",
    "             ]] \n",
    "\n",
    "df_plot.to_csv(f'{search_terms[0]}_summary.csv', index=False)\n",
    "print(f'{search_terms[0]}_summary.csv has been saved.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Full Data  \n",
    "\n",
    "- **Summary** can be found in the CSV file generated from the above code.  \n",
    "- **Full detailed information** is available in the CSV file generated from the code below.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_numbers(s):\n",
    "    if isinstance(s, str):\n",
    "        numbers = re.findall(r'\\d+', s)\n",
    "        return ', '.join(numbers)\n",
    "    return s\n",
    "\n",
    "columns_to_clean = ['LIR_indice_A', 'LIR_indice_B', 'cLIR_indice_A', 'cLIR_indice_B']\n",
    "\n",
    "for col in columns_to_clean:\n",
    "    df_combined[col] = df_combined[col].apply(extract_numbers)\n",
    "\n",
    "def process_list_column(value):\n",
    "    if isinstance(value, str):\n",
    "        combined_set = set()\n",
    "        # Replace '[' and ']' with '', then split by commas and convert to integers\n",
    "        value = value.replace('[', '').replace(']', '')\n",
    "        for part in value.split(','):\n",
    "            part = part.strip()\n",
    "            if part and part != '...':\n",
    "                combined_set.update(map(int, part.split()))\n",
    "        return ','.join(map(str, sorted(combined_set)))\n",
    "    return value\n",
    "\n",
    "\n",
    "df_new = df_combined.copy()\n",
    "\n",
    "columns_to_clean = ['LIR_indice_A', 'LIR_indice_B', 'cLIR_indice_A', 'cLIR_indice_B']\n",
    "\n",
    "# Define the columns that contain lists\n",
    "list_columns = ['LIR_indice_A', 'LIR_indice_B', 'cLIR_indice_A', 'cLIR_indice_B']\n",
    "\n",
    "for col in columns_to_clean:\n",
    "    df_new[col] = df_new[col].apply(lambda x: re.sub(r'\\[\\s+', '[', x) if isinstance(x, str) else x)    \n",
    "    \n",
    "# Post-process list columns to combine all unique coordinates\n",
    "for col in list_columns:\n",
    "    df_new[col] = df_new[col].apply(process_list_column)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the original mapping (short -> full name)\n",
    "column_mapping_reverse = {v: k for k, v in column_mapping.items()}\n",
    "\n",
    "# Define README column mapping section\n",
    "readme_column_data = pd.DataFrame([\n",
    "    {'Column Name': short, 'Full Description': full}\n",
    "    for short, full in column_mapping_reverse.items()\n",
    "])\n",
    "\n",
    "# Define the yeast-fly-human reference PPI benchmark metrics table\n",
    "benchmark_metrics_data = pd.DataFrame([\n",
    "    ['Total', 'iLIS', 0.22252179, 0.735465116, 0.101574803, 0.338551792, 0.688953488, 0.052755906, 0.550663967, 0.406976744, 0.011023622, 0.291922729, 0.720930233, 0.063779528],\n",
    "    ['Total', 'LIS', 0.168237963, 0.75872093, 0.101574803, 0.256569551, 0.686046512, 0.051968504, 0.438797151, 0.39244186, 0.01023622, 0.203428117, 0.73255814, 0.070866142],\n",
    "    ['Total', 'cLIS', 0.297521879, 0.735465116, 0.103149606, 0.4640458, 0.680232558, 0.050393701, 0.715617835, 0.39244186, 0.01023622, 0.21830786, 0.787790698, 0.134645669],\n",
    "    ['Total', 'ipTM', 0.48, 0.639534884, 0.105511811, 0.59, 0.520348837, 0.051968504, 0.72, 0.36627907, 0.011023622, 0.43, 0.683139535, 0.133070866],\n",
    "    ['Total', 'Confidence', 0.494, 0.610465116, 0.1, 0.576, 0.511627907, 0.050393701, 0.684, 0.363372093, 0.01023622, 0.41, 0.700581395, 0.172440945]\n",
    "], columns=[\n",
    "    'Group', 'Metric', 'Threshold@0.10', 'TPR@0.10', 'FPR@0.10', \n",
    "    'Threshold@0.05', 'TPR@0.05', 'FPR@0.05', 'Threshold@0.01', \n",
    "    'TPR@0.01', 'FPR@0.01', 'Youden_Threshold', 'Youden_TPR', 'Youden_FPR'\n",
    "])\n",
    "\n",
    "# Reordering the columns\n",
    "desired_order = ['Protein_1', 'Protein_2', 'Rank', 'iLIS', 'LIS', 'cLIS', 'ipTM', 'Confidence', 'pLDDT', 'pTM']\n",
    "remaining_columns = [col for col in df_new.columns if col not in desired_order]\n",
    "new_column_order = desired_order + remaining_columns\n",
    "\n",
    "# Reorder the dataframe\n",
    "df_new = df_new[new_column_order]\n",
    "\n",
    "# Define iLIS explanation with only the bold title\n",
    "ilis_explanation = pd.DataFrame([\n",
    "    [\"iLIS = sqrt(LIS * cLIS)\"],\n",
    "    [\"\"]\n",
    "], columns=[\"Description\"])\n",
    "\n",
    "# Save to Excel and apply formatting\n",
    "with pd.ExcelWriter(f'{search_terms[0]}_combined.xlsx', engine='xlsxwriter') as writer:\n",
    "    df_new.to_excel(writer, sheet_name='Data', index=False)\n",
    "\n",
    "    # Access the workbook and Data worksheet\n",
    "    workbook = writer.book\n",
    "    data_worksheet = writer.sheets['Data']\n",
    "    \n",
    "    # Apply wrapped text format for column headers\n",
    "    wrap_format = workbook.add_format({'text_wrap': True, 'bold': True, 'align': 'center', 'valign': 'vcenter'})\n",
    "\n",
    "    # Set column widths and apply wrap format\n",
    "    for col_num, value in enumerate(df_new.columns):\n",
    "        # data_worksheet.write(0, col_num, value, wrap_format)\n",
    "        data_worksheet.set_column(col_num, col_num, 9)\n",
    "    \n",
    "    # Write README sheet\n",
    "    workbook = writer.book\n",
    "    worksheet = workbook.add_worksheet('README')\n",
    "    writer.sheets['README'] = worksheet\n",
    "\n",
    "    # Define bold format\n",
    "    bold_format = workbook.add_format({'bold': True})\n",
    "\n",
    "    # Write bold title\n",
    "    worksheet.write(0, 0, \"integrated LIS (iLIS) Calculation\", bold_format)\n",
    "\n",
    "    # Write formula below title\n",
    "    worksheet.write(1, 0, \"iLIS = sqrt(LIS * cLIS)\")\n",
    "\n",
    "    # Write column mapping below explanation\n",
    "    readme_column_data.to_excel(writer, sheet_name='README', index=False, startrow=3)\n",
    "\n",
    "    # Write performance metrics below column mapping\n",
    "    benchmark_metrics_data.to_excel(writer, sheet_name='README', index=False, startrow=len(readme_column_data) + 6)\n",
    "\n",
    "print(f'{search_terms[0]}_combined.xlsx with README containing benchmark results for each metric created.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
